{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create and return a vocabulary as a list of word types \n",
    "with counts >= cutoff in the training directory\n",
    "@param training_directory - the path to the training directory\n",
    "@param cuttoff - the cutoff in the vocabulary\n",
    "@return the vocabulary in the form of a dictionary\n",
    "\"\"\"\n",
    "def create_vocabulary(training_directory, cutoff):\n",
    "    vocab = dict()\n",
    "    \n",
    "    # Get every file in the directory\n",
    "    sub_dir = os.listdir(training_directory)\n",
    "\n",
    "    # Go through each branch of the main directory\n",
    "    for sub in sub_dir:\n",
    "        # Join the path of the directory with the current folder\n",
    "        sub = os.path.join(training_directory, sub)\n",
    "        # Get all the training set in that sub directory\n",
    "        training_set = os.listdir(sub)\n",
    "        # Go through every training set to add new words count the frequency\n",
    "        for file in training_set:\n",
    "            # Join the path to access the file\n",
    "            with open(os.path.join(sub, file), 'r', errors='ignore') as f:\n",
    "                for i in f:\n",
    "                    # Get the word in each line\n",
    "                    i = i.split('\\n')\n",
    "                    i = i[0]\n",
    "                    \n",
    "                    # If it is already in the vocabulary, increase the frequency\n",
    "                    if i in vocab:\n",
    "                        vocab[i] += 1\n",
    "                    # Add that word in otherwise\n",
    "                    else:\n",
    "                        vocab[i] = 1\n",
    "    \n",
    "    # Remove words have frequency lower than the cutoff if the cutoff is not 1\n",
    "    if cutoff>1:\n",
    "        key = list(vocab.keys())\n",
    "        for word in key:\n",
    "            if vocab[word]<cutoff:\n",
    "                del vocab[word]\n",
    "\n",
    "    # Create and return the vocab\n",
    "    vocab = list(vocab.keys())\n",
    "    vocab = sorted(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create and return a bag of words Python dictionary from a single document\n",
    "@param vocab - the vocabulary\n",
    "@param filepath - the path to the document\n",
    "@return a bag of word in the form of a dictionary\n",
    "\"\"\"\n",
    "def create_bow(vocab, filepath):\n",
    "    bow = dict()\n",
    "    # Initialize an element to store the frequency of OOV\n",
    "    bow[None] = 0\n",
    "    \n",
    "    # Go through the file\n",
    "    with open(filepath, 'r', errors='ignore') as f:\n",
    "        for i in f:\n",
    "            # Get the word in each line\n",
    "            i = i.split('\\n')\n",
    "            i = i[0]\n",
    "            \n",
    "            # Categorize the word or increment the count\n",
    "            if i in bow:\n",
    "                bow[i] += 1\n",
    "            elif i in vocab:\n",
    "                bow[i] = 1\n",
    "            # If the word is not in the vocab, increment the OOV count\n",
    "            else:\n",
    "                bow[None] += 1\n",
    "                \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create and return training set (bag of words Python dictionary + label) \n",
    "from the files in a training directory\n",
    "@param vocab - the vocabulary\n",
    "@param directory - the directory of the training data\n",
    "\"\"\"\n",
    "def load_training_data(vocab, directory):\n",
    "    training_data = []\n",
    "    # Get all the sub training directory\n",
    "    sub_dir = os.listdir(directory)\n",
    "\n",
    "    # Go through each sub file\n",
    "    for sub in sub_dir:\n",
    "        # Get the label of the training set\n",
    "        label = str(sub);\n",
    "        # Get the path to the sub\n",
    "        sub = os.path.join(directory, sub)\n",
    "        # Get all training set\n",
    "        training_set = os.listdir(sub)\n",
    "        # Append the training data\n",
    "        for file in training_set:\n",
    "            training_data.append({'label':label, 'bow':create_bow(vocab,os.path.join(sub, file))})\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "given a training set, estimate and return \n",
    "the prior probability p(label) of each label\n",
    "@param training_data - the processed training data\n",
    "@param label_list - the list of label for prediction\n",
    "@return the prior probability\n",
    "\"\"\"\n",
    "def prior(training_data, label_list):\n",
    "    prob = dict()\n",
    "    \n",
    "    # Initialize the probability for each label\n",
    "    for i in label_list:\n",
    "        prob[i]=0\n",
    "    \n",
    "    # Get the count for each label\n",
    "    for i in training_data:\n",
    "        label = str(i['label'])\n",
    "        if label in prob:\n",
    "            prob[label] += 1\n",
    "    \n",
    "    # Divide by the total number of data and take the log\n",
    "    for i in prob:\n",
    "        prob[i] /= len(training_data)\n",
    "        prob[i] = math.log(prob[i])\n",
    "        \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "given a training set and a vocabulary, estimate and return \n",
    "the class conditional distribution P(word|label) over all words\n",
    "@param vocab - the vocabulary\n",
    "@param training_data - the processed training data\n",
    "@param label - the label needs predicting the probability\n",
    "@return the probability of words for each label\n",
    "\"\"\"\n",
    "def p_word_given_label(vocab, training_data, label):\n",
    "    # Initialize the count for each word in the vocab\n",
    "    p_word = dict(zip(vocab, [0]*len(vocab)))\n",
    "    # Initialize the count for OOV\n",
    "    p_word[None] = 0\n",
    "    # Smoothing by initialize the sum equal the total of words in the vocab plus OOV\n",
    "    sum_word = len(vocab)+1\n",
    "    \n",
    "    # Count the frequency of each word in a particular label\n",
    "    for i in training_data:\n",
    "        if i['label']==label:\n",
    "            for j in i['bow']:\n",
    "                count = i['bow'][j]\n",
    "                if j in p_word:\n",
    "                    p_word[j]+=count\n",
    "                # If the word is not in the vocab, increment OOV count\n",
    "                else:\n",
    "                    p_word[None]+=count\n",
    "                # Add the word count to the total number of words\n",
    "                sum_word += count\n",
    "    \n",
    "    # Calculate P(word|label) with smoothing\n",
    "    for i in p_word:\n",
    "        p_word[i] = math.log((p_word[i]+1)/sum_word)\n",
    "    \n",
    "    return p_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "loads the training data, estimates the prior distribution P(label)\n",
    "and class conditional distributions P(word|label), return the trained model\n",
    "@param training_directory - the path to the training directory\n",
    "@param cutoff - the cut off to create the vocabulary\n",
    "@return the trained model\n",
    "\"\"\"\n",
    "def train(training_directory, cutoff):\n",
    "    # Create the vocabulary for the training\n",
    "    vocab = create_vocabulary(training_directory, cutoff)\n",
    "    \n",
    "    # Load the training data\n",
    "    training_data = load_training_data(vocab, training_directory)\n",
    "    \n",
    "    # Calculate the log prior\n",
    "    log_prior = prior(training_data, ['2016','2020'])\n",
    "    \n",
    "    # Calculate the P(w|label) for each label\n",
    "    p_w_2016 = p_word_given_label(vocab, training_data, '2016')\n",
    "    p_w_2020 = p_word_given_label(vocab, training_data, '2020')\n",
    "    \n",
    "    return {'vocabulary': vocab,\n",
    "           'log prior': log_prior,\n",
    "           'log p(w|2016)': p_w_2016,\n",
    "           'log p(w|2020)': p_w_2020}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "given a trained model, predict the label for the test document\n",
    "@param model - the trained model\n",
    "@param filepath - the path to the file needs classifying\n",
    "@return the prediction and the probability for each label\n",
    "\"\"\"\n",
    "def classify(model, filepath):\n",
    "    # Create a bow for the file\n",
    "    bow = create_bow(model['vocabulary'], filepath)\n",
    "    \n",
    "    # Initialize the probability f(label) and the prediction y\n",
    "    f_2016 = model['log prior']['2016']\n",
    "    f_2020 = model['log prior']['2020']\n",
    "    y = ''\n",
    "    \n",
    "    # Calculate the probability\n",
    "    for i in bow:\n",
    "        f_2016 += model['log p(w|2016)'][i]*bow[i]\n",
    "        f_2020 += model['log p(w|2020)'][i]*bow[i]\n",
    "        \n",
    "    # Predict based on the calculated probability\n",
    "    if f_2016>f_2020:\n",
    "        y = '2016'\n",
    "    else:\n",
    "        y = '2020'\n",
    "    \n",
    "    return {'predicted y': y, 'log p(y=2016|x)': f_2016, 'log p(y=2020|x)': f_2020}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocabulary': ['.', 'a'],\n",
       " 'log prior': {'2016': -0.40546510810816444, '2020': -1.0986122886681098},\n",
       " 'log p(w|2016)': {'.': -1.7047480922384253,\n",
       "  'a': -1.2992829841302609,\n",
       "  None: -0.6061358035703156},\n",
       " 'log p(w|2020)': {'.': -1.6094379124341003,\n",
       "  'a': -2.3025850929940455,\n",
       "  None: -0.35667494393873245}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train('./EasyFiles/', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predicted y': '2020',\n",
       " 'log p(y=2016|x)': -3916.464890789782,\n",
       " 'log p(y=2020|x)': -3906.349624562405}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = train('./corpus/training/', 2)\n",
    "classify(model, './corpus/test/2016/0.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
